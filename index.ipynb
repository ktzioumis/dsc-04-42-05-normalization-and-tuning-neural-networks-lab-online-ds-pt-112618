{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization and Tuning Neural Networks - Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "For this lab on initialization and optimization, let's look at a slightly different type of neural network. This time, we will not perform a classification task as we've done before (Santa vs not santa, bank complaint types), but we'll look at a linear regression problem.\n",
    "\n",
    "We can just as well use deep learning networks for linear regression as for a classification problem. Do note that getting regression to work with neural networks is a hard problem because the output is unbounded ($\\hat y$ can technically range from $-\\infty$ to $+\\infty$, and the models are especially prone to exploding gradients. This issue makes a regression exercise the perfect learning case!\n",
    "\n",
    "## Objectives\n",
    "You will be able to:\n",
    "* Build a nueral network using keras\n",
    "* Normalize your data to assist algorithm convergence\n",
    "* Implement and observe the impact of various initialization techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras import initializers\n",
    "from keras import layers\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import preprocessing\n",
    "from keras import optimizers\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we'll be working with is data related to facebook posts published during the year of 2014 on the Facebook's page of a renowned cosmetics brand.  It includes 7 features known prior to post publication, and 12 features for evaluating the post impact. What we want to do is make a predictor for the number of \"likes\" for a post, taking into account the 7 features prior to posting.\n",
    "\n",
    "First, let's import the data set and delete any rows with missing data. Afterwards, briefly preview the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Page total likes</th>\n",
       "      <th>Type</th>\n",
       "      <th>Category</th>\n",
       "      <th>Post Month</th>\n",
       "      <th>Post Weekday</th>\n",
       "      <th>Post Hour</th>\n",
       "      <th>Paid</th>\n",
       "      <th>Lifetime Post Total Reach</th>\n",
       "      <th>Lifetime Post Total Impressions</th>\n",
       "      <th>Lifetime Engaged Users</th>\n",
       "      <th>Lifetime Post Consumers</th>\n",
       "      <th>Lifetime Post Consumptions</th>\n",
       "      <th>Lifetime Post Impressions by people who have liked your Page</th>\n",
       "      <th>Lifetime Post reach by people who like your Page</th>\n",
       "      <th>Lifetime People who have liked your Page and engaged with your post</th>\n",
       "      <th>comment</th>\n",
       "      <th>like</th>\n",
       "      <th>share</th>\n",
       "      <th>Total Interactions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>139441</td>\n",
       "      <td>Photo</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2752</td>\n",
       "      <td>5091</td>\n",
       "      <td>178</td>\n",
       "      <td>109</td>\n",
       "      <td>159</td>\n",
       "      <td>3078</td>\n",
       "      <td>1640</td>\n",
       "      <td>119</td>\n",
       "      <td>4</td>\n",
       "      <td>79.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>139441</td>\n",
       "      <td>Status</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10460</td>\n",
       "      <td>19057</td>\n",
       "      <td>1457</td>\n",
       "      <td>1361</td>\n",
       "      <td>1674</td>\n",
       "      <td>11710</td>\n",
       "      <td>6112</td>\n",
       "      <td>1108</td>\n",
       "      <td>5</td>\n",
       "      <td>130.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>139441</td>\n",
       "      <td>Photo</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2413</td>\n",
       "      <td>4373</td>\n",
       "      <td>177</td>\n",
       "      <td>113</td>\n",
       "      <td>154</td>\n",
       "      <td>2812</td>\n",
       "      <td>1503</td>\n",
       "      <td>132</td>\n",
       "      <td>0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>139441</td>\n",
       "      <td>Photo</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50128</td>\n",
       "      <td>87991</td>\n",
       "      <td>2211</td>\n",
       "      <td>790</td>\n",
       "      <td>1119</td>\n",
       "      <td>61027</td>\n",
       "      <td>32048</td>\n",
       "      <td>1386</td>\n",
       "      <td>58</td>\n",
       "      <td>1572.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>1777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>139441</td>\n",
       "      <td>Photo</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7244</td>\n",
       "      <td>13594</td>\n",
       "      <td>671</td>\n",
       "      <td>410</td>\n",
       "      <td>580</td>\n",
       "      <td>6228</td>\n",
       "      <td>3200</td>\n",
       "      <td>396</td>\n",
       "      <td>19</td>\n",
       "      <td>325.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Page total likes    Type  Category  Post Month  Post Weekday  Post Hour  \\\n",
       "0            139441   Photo         2          12             4          3   \n",
       "1            139441  Status         2          12             3         10   \n",
       "2            139441   Photo         3          12             3          3   \n",
       "3            139441   Photo         2          12             2         10   \n",
       "4            139441   Photo         2          12             2          3   \n",
       "\n",
       "   Paid  Lifetime Post Total Reach  Lifetime Post Total Impressions  \\\n",
       "0   0.0                       2752                             5091   \n",
       "1   0.0                      10460                            19057   \n",
       "2   0.0                       2413                             4373   \n",
       "3   1.0                      50128                            87991   \n",
       "4   0.0                       7244                            13594   \n",
       "\n",
       "   Lifetime Engaged Users  Lifetime Post Consumers  \\\n",
       "0                     178                      109   \n",
       "1                    1457                     1361   \n",
       "2                     177                      113   \n",
       "3                    2211                      790   \n",
       "4                     671                      410   \n",
       "\n",
       "   Lifetime Post Consumptions  \\\n",
       "0                         159   \n",
       "1                        1674   \n",
       "2                         154   \n",
       "3                        1119   \n",
       "4                         580   \n",
       "\n",
       "   Lifetime Post Impressions by people who have liked your Page  \\\n",
       "0                                               3078              \n",
       "1                                              11710              \n",
       "2                                               2812              \n",
       "3                                              61027              \n",
       "4                                               6228              \n",
       "\n",
       "   Lifetime Post reach by people who like your Page  \\\n",
       "0                                              1640   \n",
       "1                                              6112   \n",
       "2                                              1503   \n",
       "3                                             32048   \n",
       "4                                              3200   \n",
       "\n",
       "   Lifetime People who have liked your Page and engaged with your post  \\\n",
       "0                                                119                     \n",
       "1                                               1108                     \n",
       "2                                                132                     \n",
       "3                                               1386                     \n",
       "4                                                396                     \n",
       "\n",
       "   comment    like  share  Total Interactions  \n",
       "0        4    79.0   17.0                 100  \n",
       "1        5   130.0   29.0                 164  \n",
       "2        0    66.0   14.0                  80  \n",
       "3       58  1572.0  147.0                1777  \n",
       "4       19   325.0   49.0                 393  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your code here; load the dataset and drop rows with missing values. Then preview the data.\n",
    "df=pd.read_csv('dataset_Facebook.csv',sep=';')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Page total likes</th>\n",
       "      <th>Category</th>\n",
       "      <th>Post Month</th>\n",
       "      <th>Post Weekday</th>\n",
       "      <th>Post Hour</th>\n",
       "      <th>Paid</th>\n",
       "      <th>Lifetime Post Total Reach</th>\n",
       "      <th>Lifetime Post Total Impressions</th>\n",
       "      <th>Lifetime Engaged Users</th>\n",
       "      <th>Lifetime Post Consumers</th>\n",
       "      <th>Lifetime Post Consumptions</th>\n",
       "      <th>Lifetime Post Impressions by people who have liked your Page</th>\n",
       "      <th>Lifetime Post reach by people who like your Page</th>\n",
       "      <th>Lifetime People who have liked your Page and engaged with your post</th>\n",
       "      <th>comment</th>\n",
       "      <th>like</th>\n",
       "      <th>share</th>\n",
       "      <th>Total Interactions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>499.000000</td>\n",
       "      <td>500.00000</td>\n",
       "      <td>5.000000e+02</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>5.000000e+02</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.00000</td>\n",
       "      <td>499.000000</td>\n",
       "      <td>496.000000</td>\n",
       "      <td>500.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>123194.176000</td>\n",
       "      <td>1.880000</td>\n",
       "      <td>7.038000</td>\n",
       "      <td>4.150000</td>\n",
       "      <td>7.840000</td>\n",
       "      <td>0.278557</td>\n",
       "      <td>13903.36000</td>\n",
       "      <td>2.958595e+04</td>\n",
       "      <td>920.344000</td>\n",
       "      <td>798.772000</td>\n",
       "      <td>1415.130000</td>\n",
       "      <td>1.676638e+04</td>\n",
       "      <td>6585.488000</td>\n",
       "      <td>609.986000</td>\n",
       "      <td>7.48200</td>\n",
       "      <td>177.945892</td>\n",
       "      <td>27.266129</td>\n",
       "      <td>212.120000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>16272.813214</td>\n",
       "      <td>0.852675</td>\n",
       "      <td>3.307936</td>\n",
       "      <td>2.030701</td>\n",
       "      <td>4.368589</td>\n",
       "      <td>0.448739</td>\n",
       "      <td>22740.78789</td>\n",
       "      <td>7.680325e+04</td>\n",
       "      <td>985.016636</td>\n",
       "      <td>882.505013</td>\n",
       "      <td>2000.594118</td>\n",
       "      <td>5.979102e+04</td>\n",
       "      <td>7682.009405</td>\n",
       "      <td>612.725618</td>\n",
       "      <td>21.18091</td>\n",
       "      <td>323.398742</td>\n",
       "      <td>42.613292</td>\n",
       "      <td>380.233118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>81370.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>238.00000</td>\n",
       "      <td>5.700000e+02</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>5.670000e+02</td>\n",
       "      <td>236.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>112676.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3315.00000</td>\n",
       "      <td>5.694750e+03</td>\n",
       "      <td>393.750000</td>\n",
       "      <td>332.500000</td>\n",
       "      <td>509.250000</td>\n",
       "      <td>3.969750e+03</td>\n",
       "      <td>2181.500000</td>\n",
       "      <td>291.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>56.500000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>71.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>129600.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5281.00000</td>\n",
       "      <td>9.051000e+03</td>\n",
       "      <td>625.500000</td>\n",
       "      <td>551.500000</td>\n",
       "      <td>851.000000</td>\n",
       "      <td>6.255500e+03</td>\n",
       "      <td>3417.000000</td>\n",
       "      <td>412.000000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>123.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>136393.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>13168.00000</td>\n",
       "      <td>2.208550e+04</td>\n",
       "      <td>1062.000000</td>\n",
       "      <td>955.500000</td>\n",
       "      <td>1463.000000</td>\n",
       "      <td>1.486050e+04</td>\n",
       "      <td>7989.000000</td>\n",
       "      <td>656.250000</td>\n",
       "      <td>7.00000</td>\n",
       "      <td>187.500000</td>\n",
       "      <td>32.250000</td>\n",
       "      <td>228.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>139441.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>180480.00000</td>\n",
       "      <td>1.110282e+06</td>\n",
       "      <td>11452.000000</td>\n",
       "      <td>11328.000000</td>\n",
       "      <td>19779.000000</td>\n",
       "      <td>1.107833e+06</td>\n",
       "      <td>51456.000000</td>\n",
       "      <td>4376.000000</td>\n",
       "      <td>372.00000</td>\n",
       "      <td>5172.000000</td>\n",
       "      <td>790.000000</td>\n",
       "      <td>6334.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Page total likes    Category  Post Month  Post Weekday   Post Hour  \\\n",
       "count        500.000000  500.000000  500.000000    500.000000  500.000000   \n",
       "mean      123194.176000    1.880000    7.038000      4.150000    7.840000   \n",
       "std        16272.813214    0.852675    3.307936      2.030701    4.368589   \n",
       "min        81370.000000    1.000000    1.000000      1.000000    1.000000   \n",
       "25%       112676.000000    1.000000    4.000000      2.000000    3.000000   \n",
       "50%       129600.000000    2.000000    7.000000      4.000000    9.000000   \n",
       "75%       136393.000000    3.000000   10.000000      6.000000   11.000000   \n",
       "max       139441.000000    3.000000   12.000000      7.000000   23.000000   \n",
       "\n",
       "             Paid  Lifetime Post Total Reach  Lifetime Post Total Impressions  \\\n",
       "count  499.000000                  500.00000                     5.000000e+02   \n",
       "mean     0.278557                13903.36000                     2.958595e+04   \n",
       "std      0.448739                22740.78789                     7.680325e+04   \n",
       "min      0.000000                  238.00000                     5.700000e+02   \n",
       "25%      0.000000                 3315.00000                     5.694750e+03   \n",
       "50%      0.000000                 5281.00000                     9.051000e+03   \n",
       "75%      1.000000                13168.00000                     2.208550e+04   \n",
       "max      1.000000               180480.00000                     1.110282e+06   \n",
       "\n",
       "       Lifetime Engaged Users  Lifetime Post Consumers  \\\n",
       "count              500.000000               500.000000   \n",
       "mean               920.344000               798.772000   \n",
       "std                985.016636               882.505013   \n",
       "min                  9.000000                 9.000000   \n",
       "25%                393.750000               332.500000   \n",
       "50%                625.500000               551.500000   \n",
       "75%               1062.000000               955.500000   \n",
       "max              11452.000000             11328.000000   \n",
       "\n",
       "       Lifetime Post Consumptions  \\\n",
       "count                  500.000000   \n",
       "mean                  1415.130000   \n",
       "std                   2000.594118   \n",
       "min                      9.000000   \n",
       "25%                    509.250000   \n",
       "50%                    851.000000   \n",
       "75%                   1463.000000   \n",
       "max                  19779.000000   \n",
       "\n",
       "       Lifetime Post Impressions by people who have liked your Page  \\\n",
       "count                                       5.000000e+02              \n",
       "mean                                        1.676638e+04              \n",
       "std                                         5.979102e+04              \n",
       "min                                         5.670000e+02              \n",
       "25%                                         3.969750e+03              \n",
       "50%                                         6.255500e+03              \n",
       "75%                                         1.486050e+04              \n",
       "max                                         1.107833e+06              \n",
       "\n",
       "       Lifetime Post reach by people who like your Page  \\\n",
       "count                                        500.000000   \n",
       "mean                                        6585.488000   \n",
       "std                                         7682.009405   \n",
       "min                                          236.000000   \n",
       "25%                                         2181.500000   \n",
       "50%                                         3417.000000   \n",
       "75%                                         7989.000000   \n",
       "max                                        51456.000000   \n",
       "\n",
       "       Lifetime People who have liked your Page and engaged with your post  \\\n",
       "count                                         500.000000                     \n",
       "mean                                          609.986000                     \n",
       "std                                           612.725618                     \n",
       "min                                             9.000000                     \n",
       "25%                                           291.000000                     \n",
       "50%                                           412.000000                     \n",
       "75%                                           656.250000                     \n",
       "max                                          4376.000000                     \n",
       "\n",
       "         comment         like       share  Total Interactions  \n",
       "count  500.00000   499.000000  496.000000          500.000000  \n",
       "mean     7.48200   177.945892   27.266129          212.120000  \n",
       "std     21.18091   323.398742   42.613292          380.233118  \n",
       "min      0.00000     0.000000    0.000000            0.000000  \n",
       "25%      1.00000    56.500000   10.000000           71.000000  \n",
       "50%      3.00000   101.000000   19.000000          123.500000  \n",
       "75%      7.00000   187.500000   32.250000          228.500000  \n",
       "max    372.00000  5172.000000  790.000000         6334.000000  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500 entries, 0 to 499\n",
      "Data columns (total 19 columns):\n",
      "Page total likes                                                       500 non-null int64\n",
      "Type                                                                   500 non-null object\n",
      "Category                                                               500 non-null int64\n",
      "Post Month                                                             500 non-null int64\n",
      "Post Weekday                                                           500 non-null int64\n",
      "Post Hour                                                              500 non-null int64\n",
      "Paid                                                                   499 non-null float64\n",
      "Lifetime Post Total Reach                                              500 non-null int64\n",
      "Lifetime Post Total Impressions                                        500 non-null int64\n",
      "Lifetime Engaged Users                                                 500 non-null int64\n",
      "Lifetime Post Consumers                                                500 non-null int64\n",
      "Lifetime Post Consumptions                                             500 non-null int64\n",
      "Lifetime Post Impressions by people who have liked your Page           500 non-null int64\n",
      "Lifetime Post reach by people who like your Page                       500 non-null int64\n",
      "Lifetime People who have liked your Page and engaged with your post    500 non-null int64\n",
      "comment                                                                500 non-null int64\n",
      "like                                                                   499 non-null float64\n",
      "share                                                                  496 non-null float64\n",
      "Total Interactions                                                     500 non-null int64\n",
      "dtypes: float64(3), int64(15), object(1)\n",
      "memory usage: 74.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 495 entries, 0 to 498\n",
      "Data columns (total 19 columns):\n",
      "Page total likes                                                       495 non-null int64\n",
      "Type                                                                   495 non-null object\n",
      "Category                                                               495 non-null int64\n",
      "Post Month                                                             495 non-null int64\n",
      "Post Weekday                                                           495 non-null int64\n",
      "Post Hour                                                              495 non-null int64\n",
      "Paid                                                                   495 non-null float64\n",
      "Lifetime Post Total Reach                                              495 non-null int64\n",
      "Lifetime Post Total Impressions                                        495 non-null int64\n",
      "Lifetime Engaged Users                                                 495 non-null int64\n",
      "Lifetime Post Consumers                                                495 non-null int64\n",
      "Lifetime Post Consumptions                                             495 non-null int64\n",
      "Lifetime Post Impressions by people who have liked your Page           495 non-null int64\n",
      "Lifetime Post reach by people who like your Page                       495 non-null int64\n",
      "Lifetime People who have liked your Page and engaged with your post    495 non-null int64\n",
      "comment                                                                495 non-null int64\n",
      "like                                                                   495 non-null float64\n",
      "share                                                                  495 non-null float64\n",
      "Total Interactions                                                     495 non-null int64\n",
      "dtypes: float64(3), int64(15), object(1)\n",
      "memory usage: 77.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df=df.dropna()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize the Input Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at our input data. We'll use the 7 first columns as our predictors. We'll do the following two things:\n",
    "- Normalize the continuous variables --> you can do this using `np.mean()` and `np.std()`\n",
    "- Make dummy variables of the categorical variables (you can do this by using `pd.get_dummies`)\n",
    "\n",
    "We only count \"Category\" and \"Type\" as categorical variables. Note that you can argue that \"Post month\", \"Post Weekday\" and \"Post Hour\" can also be considered categories, but we'll just treat them as being continuous for now.\n",
    "\n",
    "You'll then use these to define X and Y. \n",
    "\n",
    "To summarize, X will be:\n",
    "* Page total likes\n",
    "* Post Month\n",
    "* Post Weekday\n",
    "* Post Hour\n",
    "* Paid\n",
    "along with dummy variables for:\n",
    "* Type\n",
    "* Category\n",
    "\n",
    "\n",
    "Be sure to normalize your features by subtracting the mean and dividing by the standard deviation.  \n",
    "\n",
    "Finally, y will simply be the \"like\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Page total likes</th>\n",
       "      <th>Post Month</th>\n",
       "      <th>Post Weekday</th>\n",
       "      <th>Post Hour</th>\n",
       "      <th>Paid</th>\n",
       "      <th>Type_Link</th>\n",
       "      <th>Type_Photo</th>\n",
       "      <th>Type_Status</th>\n",
       "      <th>Type_Video</th>\n",
       "      <th>Category_1</th>\n",
       "      <th>Category_2</th>\n",
       "      <th>Category_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.00496</td>\n",
       "      <td>1.506154</td>\n",
       "      <td>-0.065724</td>\n",
       "      <td>-1.105878</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.00496</td>\n",
       "      <td>1.506154</td>\n",
       "      <td>-0.558655</td>\n",
       "      <td>0.492065</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.00496</td>\n",
       "      <td>1.506154</td>\n",
       "      <td>-0.558655</td>\n",
       "      <td>-1.105878</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.00496</td>\n",
       "      <td>1.506154</td>\n",
       "      <td>-1.051585</td>\n",
       "      <td>0.492065</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.00496</td>\n",
       "      <td>1.506154</td>\n",
       "      <td>-1.051585</td>\n",
       "      <td>-1.105878</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Page total likes  Post Month  Post Weekday  Post Hour  Paid  Type_Link  \\\n",
       "0           1.00496    1.506154     -0.065724  -1.105878   0.0          0   \n",
       "1           1.00496    1.506154     -0.558655   0.492065   0.0          0   \n",
       "2           1.00496    1.506154     -0.558655  -1.105878   0.0          0   \n",
       "3           1.00496    1.506154     -1.051585   0.492065   1.0          0   \n",
       "4           1.00496    1.506154     -1.051585  -1.105878   0.0          0   \n",
       "\n",
       "   Type_Photo  Type_Status  Type_Video  Category_1  Category_2  Category_3  \n",
       "0           1            0           0           0           1           0  \n",
       "1           0            1           0           0           1           0  \n",
       "2           1            0           0           0           0           1  \n",
       "3           1            0           0           0           1           0  \n",
       "4           1            0           0           0           1           0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your code here; define X and y.\n",
    "\n",
    "X = pd.DataFrame()\n",
    "X['Page total likes']=(df['Page total likes']-np.mean(df['Page total likes']))/np.std(df['Page total likes'])\n",
    "X['Post Month']=(df['Post Month']-np.mean(df['Post Month']))/np.std(df['Post Month'])\n",
    "X['Post Weekday']=(df['Post Weekday']-np.mean(df['Post Weekday']))/np.std(df['Post Weekday'])\n",
    "X['Post Hour']=(df['Post Hour']-np.mean(df['Post Hour']))/np.std(df['Post Hour'])\n",
    "X['Paid']=df['Paid']\n",
    "X['Type']=df['Type']\n",
    "X['Category']=df['Category'].astype('str')\n",
    "X=pd.get_dummies(X)\n",
    "Y = df['like']\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is fairly small. Let's just split the data up in a training set and a validation set!  The next three code blocks are all provided for you; have a quick review but not need to make edits!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code provided; defining training and validation sets\n",
    "data_clean = pd.concat([X, Y], axis=1)\n",
    "np.random.seed(123)\n",
    "train, validation = train_test_split(data_clean, test_size=0.2)\n",
    "\n",
    "X_val = validation.iloc[:,0:12]\n",
    "Y_val = validation.iloc[:,12]\n",
    "X_train = train.iloc[:,0:12]\n",
    "Y_train = train.iloc[:,12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\kosta\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\kosta\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "#Code provided; building an initial model\n",
    "np.random.seed(123)\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(8, input_dim=12, activation='relu'))\n",
    "model.add(layers.Dense(1, activation = 'linear'))\n",
    "\n",
    "model.compile(optimizer= \"sgd\" ,loss='mse',metrics=['mse'])\n",
    "hist = model.fit(X_train, Y_train, batch_size=32, \n",
    "                 epochs=100, validation_data = (X_val, Y_val), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[135689.81668244948,\n",
       " 121765.053286774,\n",
       " 115465.64632654672,\n",
       " 111735.93308080808,\n",
       " 109538.18884154041,\n",
       " 108165.06949376578,\n",
       " 107400.40372474748,\n",
       " 106890.81784643308,\n",
       " 106645.33341224748,\n",
       " 106513.21409406565]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Code provided; previewing the loss through successive epochs\n",
    "hist.history['loss'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you see what happend? all the values for training and validation loss are \"nan\". There could be several reasons for that, but as we already mentioned there is likely a vanishing or exploding gradient problem. recall that we normalized out inputs. But how about the outputs? Let's have a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "208     54.0\n",
       "290     23.0\n",
       "286     15.0\n",
       "0       79.0\n",
       "401    329.0\n",
       "Name: like, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, indeed. We didn't normalize them and we should, as they take pretty high values. Let\n",
    "s rerun the model but make sure that the output is normalized as well!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing the output\n",
    "\n",
    "Normalize Y as you did X by subtracting the mean and dividing by the standard deviation. Then, resplit the data into training and validation sets as we demonstrated above, and retrain a new model using your normalized X and Y data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here: redefine Y after normalizing the data.\n",
    "Y=(Y-np.mean(Y))/np.std(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; create training and validation sets as before. Use random seed 123.\n",
    "data_clean = pd.concat([X, Y], axis=1)\n",
    "np.random.seed(123)\n",
    "train, validation = train_test_split(data_clean, test_size=0.2)\n",
    "\n",
    "X_val = validation.iloc[:,0:12]\n",
    "Y_val = validation.iloc[:,12]\n",
    "X_train = train.iloc[:,0:12]\n",
    "Y_train = train.iloc[:,12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 396 samples, validate on 99 samples\n",
      "Epoch 1/100\n",
      "396/396 [==============================] - 0s 717us/step - loss: 1.4368 - mean_squared_error: 1.4368 - val_loss: 1.2757 - val_mean_squared_error: 1.2757\n",
      "Epoch 2/100\n",
      "396/396 [==============================] - 0s 51us/step - loss: 1.1738 - mean_squared_error: 1.1738 - val_loss: 1.1410 - val_mean_squared_error: 1.1410\n",
      "Epoch 3/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: 1.1074 - mean_squared_error: 1.1074 - val_loss: 1.0866 - val_mean_squared_error: 1.0866\n",
      "Epoch 4/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: 1.0811 - mean_squared_error: 1.0811 - val_loss: 1.0587 - val_mean_squared_error: 1.0587\n",
      "Epoch 5/100\n",
      "396/396 [==============================] - 0s 106us/step - loss: 1.0675 - mean_squared_error: 1.0675 - val_loss: 1.0410 - val_mean_squared_error: 1.0410\n",
      "Epoch 6/100\n",
      "396/396 [==============================] - 0s 61us/step - loss: 1.0552 - mean_squared_error: 1.0552 - val_loss: 1.0267 - val_mean_squared_error: 1.0267\n",
      "Epoch 7/100\n",
      "396/396 [==============================] - 0s 56us/step - loss: 1.0472 - mean_squared_error: 1.0472 - val_loss: 1.0165 - val_mean_squared_error: 1.0165\n",
      "Epoch 8/100\n",
      "396/396 [==============================] - 0s 71us/step - loss: 1.0402 - mean_squared_error: 1.0402 - val_loss: 1.0078 - val_mean_squared_error: 1.0078\n",
      "Epoch 9/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 1.0352 - mean_squared_error: 1.0352 - val_loss: 1.0005 - val_mean_squared_error: 1.0005\n",
      "Epoch 10/100\n",
      "396/396 [==============================] - 0s 106us/step - loss: 1.0306 - mean_squared_error: 1.0306 - val_loss: 0.9968 - val_mean_squared_error: 0.9968\n",
      "Epoch 11/100\n",
      "396/396 [==============================] - 0s 61us/step - loss: 1.0268 - mean_squared_error: 1.0268 - val_loss: 0.9920 - val_mean_squared_error: 0.9920\n",
      "Epoch 12/100\n",
      "396/396 [==============================] - 0s 61us/step - loss: 1.0228 - mean_squared_error: 1.0228 - val_loss: 0.9893 - val_mean_squared_error: 0.9893\n",
      "Epoch 13/100\n",
      "396/396 [==============================] - 0s 63us/step - loss: 1.0198 - mean_squared_error: 1.0198 - val_loss: 0.9846 - val_mean_squared_error: 0.9846\n",
      "Epoch 14/100\n",
      "396/396 [==============================] - 0s 58us/step - loss: 1.0175 - mean_squared_error: 1.0175 - val_loss: 0.9791 - val_mean_squared_error: 0.9791\n",
      "Epoch 15/100\n",
      "396/396 [==============================] - 0s 91us/step - loss: 1.0146 - mean_squared_error: 1.0146 - val_loss: 0.9776 - val_mean_squared_error: 0.9776\n",
      "Epoch 16/100\n",
      "396/396 [==============================] - 0s 81us/step - loss: 1.0121 - mean_squared_error: 1.0121 - val_loss: 0.9752 - val_mean_squared_error: 0.9752\n",
      "Epoch 17/100\n",
      "396/396 [==============================] - 0s 66us/step - loss: 1.0099 - mean_squared_error: 1.0099 - val_loss: 0.9736 - val_mean_squared_error: 0.9736\n",
      "Epoch 18/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 1.0082 - mean_squared_error: 1.0082 - val_loss: 0.9731 - val_mean_squared_error: 0.9731\n",
      "Epoch 19/100\n",
      "396/396 [==============================] - 0s 83us/step - loss: 1.0061 - mean_squared_error: 1.0061 - val_loss: 0.9728 - val_mean_squared_error: 0.9728\n",
      "Epoch 20/100\n",
      "396/396 [==============================] - 0s 68us/step - loss: 1.0049 - mean_squared_error: 1.0049 - val_loss: 0.9699 - val_mean_squared_error: 0.9699\n",
      "Epoch 21/100\n",
      "396/396 [==============================] - 0s 68us/step - loss: 1.0038 - mean_squared_error: 1.0038 - val_loss: 0.9646 - val_mean_squared_error: 0.9646\n",
      "Epoch 22/100\n",
      "396/396 [==============================] - 0s 66us/step - loss: 1.0022 - mean_squared_error: 1.0022 - val_loss: 0.9604 - val_mean_squared_error: 0.9604\n",
      "Epoch 23/100\n",
      "396/396 [==============================] - 0s 71us/step - loss: 1.0003 - mean_squared_error: 1.0003 - val_loss: 0.9607 - val_mean_squared_error: 0.9607\n",
      "Epoch 24/100\n",
      "396/396 [==============================] - 0s 78us/step - loss: 0.9986 - mean_squared_error: 0.9986 - val_loss: 0.9636 - val_mean_squared_error: 0.9636\n",
      "Epoch 25/100\n",
      "396/396 [==============================] - 0s 56us/step - loss: 0.9973 - mean_squared_error: 0.9973 - val_loss: 0.9649 - val_mean_squared_error: 0.9649\n",
      "Epoch 26/100\n",
      "396/396 [==============================] - 0s 61us/step - loss: 0.9968 - mean_squared_error: 0.9968 - val_loss: 0.9654 - val_mean_squared_error: 0.9654\n",
      "Epoch 27/100\n",
      "396/396 [==============================] - 0s 63us/step - loss: 0.9946 - mean_squared_error: 0.9946 - val_loss: 0.9636 - val_mean_squared_error: 0.9636\n",
      "Epoch 28/100\n",
      "396/396 [==============================] - 0s 96us/step - loss: 0.9935 - mean_squared_error: 0.9935 - val_loss: 0.9632 - val_mean_squared_error: 0.9632\n",
      "Epoch 29/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: 0.9929 - mean_squared_error: 0.9929 - val_loss: 0.9615 - val_mean_squared_error: 0.9615\n",
      "Epoch 30/100\n",
      "396/396 [==============================] - 0s 48us/step - loss: 0.9928 - mean_squared_error: 0.9928 - val_loss: 0.9561 - val_mean_squared_error: 0.9561\n",
      "Epoch 31/100\n",
      "396/396 [==============================] - 0s 83us/step - loss: 0.9905 - mean_squared_error: 0.9905 - val_loss: 0.9585 - val_mean_squared_error: 0.9585\n",
      "Epoch 32/100\n",
      "396/396 [==============================] - 0s 86us/step - loss: 0.9893 - mean_squared_error: 0.9893 - val_loss: 0.9597 - val_mean_squared_error: 0.9597\n",
      "Epoch 33/100\n",
      "396/396 [==============================] - 0s 66us/step - loss: 0.9888 - mean_squared_error: 0.9888 - val_loss: 0.9597 - val_mean_squared_error: 0.9597\n",
      "Epoch 34/100\n",
      "396/396 [==============================] - 0s 68us/step - loss: 0.9877 - mean_squared_error: 0.9877 - val_loss: 0.9595 - val_mean_squared_error: 0.9595\n",
      "Epoch 35/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 0.9866 - mean_squared_error: 0.9866 - val_loss: 0.9594 - val_mean_squared_error: 0.9594\n",
      "Epoch 36/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: 0.9858 - mean_squared_error: 0.9858 - val_loss: 0.9575 - val_mean_squared_error: 0.9575\n",
      "Epoch 37/100\n",
      "396/396 [==============================] - 0s 68us/step - loss: 0.9851 - mean_squared_error: 0.9851 - val_loss: 0.9574 - val_mean_squared_error: 0.9574\n",
      "Epoch 38/100\n",
      "396/396 [==============================] - 0s 61us/step - loss: 0.9846 - mean_squared_error: 0.9846 - val_loss: 0.9540 - val_mean_squared_error: 0.9540\n",
      "Epoch 39/100\n",
      "396/396 [==============================] - 0s 83us/step - loss: 0.9834 - mean_squared_error: 0.9834 - val_loss: 0.9533 - val_mean_squared_error: 0.9533\n",
      "Epoch 40/100\n",
      "396/396 [==============================] - 0s 53us/step - loss: 0.9825 - mean_squared_error: 0.9825 - val_loss: 0.9494 - val_mean_squared_error: 0.9494\n",
      "Epoch 41/100\n",
      "396/396 [==============================] - 0s 66us/step - loss: 0.9820 - mean_squared_error: 0.9820 - val_loss: 0.9519 - val_mean_squared_error: 0.9519\n",
      "Epoch 42/100\n",
      "396/396 [==============================] - 0s 83us/step - loss: 0.9813 - mean_squared_error: 0.9813 - val_loss: 0.9493 - val_mean_squared_error: 0.9493\n",
      "Epoch 43/100\n",
      "396/396 [==============================] - 0s 66us/step - loss: 0.9807 - mean_squared_error: 0.9807 - val_loss: 0.9513 - val_mean_squared_error: 0.9513\n",
      "Epoch 44/100\n",
      "396/396 [==============================] - 0s 63us/step - loss: 0.9807 - mean_squared_error: 0.9807 - val_loss: 0.9531 - val_mean_squared_error: 0.9531\n",
      "Epoch 45/100\n",
      "396/396 [==============================] - 0s 81us/step - loss: 0.9791 - mean_squared_error: 0.9791 - val_loss: 0.9530 - val_mean_squared_error: 0.9530\n",
      "Epoch 46/100\n",
      "396/396 [==============================] - 0s 66us/step - loss: 0.9788 - mean_squared_error: 0.9788 - val_loss: 0.9477 - val_mean_squared_error: 0.9477\n",
      "Epoch 47/100\n",
      "396/396 [==============================] - 0s 61us/step - loss: 0.9779 - mean_squared_error: 0.9779 - val_loss: 0.9480 - val_mean_squared_error: 0.9480\n",
      "Epoch 48/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 0.9771 - mean_squared_error: 0.9771 - val_loss: 0.9486 - val_mean_squared_error: 0.9486\n",
      "Epoch 49/100\n",
      "396/396 [==============================] - 0s 58us/step - loss: 0.9769 - mean_squared_error: 0.9769 - val_loss: 0.9495 - val_mean_squared_error: 0.9495\n",
      "Epoch 50/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "396/396 [==============================] - 0s 81us/step - loss: 0.9762 - mean_squared_error: 0.9762 - val_loss: 0.9483 - val_mean_squared_error: 0.9483\n",
      "Epoch 51/100\n",
      "396/396 [==============================] - 0s 61us/step - loss: 0.9757 - mean_squared_error: 0.9757 - val_loss: 0.9471 - val_mean_squared_error: 0.9471\n",
      "Epoch 52/100\n",
      "396/396 [==============================] - 0s 51us/step - loss: 0.9747 - mean_squared_error: 0.9747 - val_loss: 0.9444 - val_mean_squared_error: 0.9444\n",
      "Epoch 53/100\n",
      "396/396 [==============================] - 0s 58us/step - loss: 0.9746 - mean_squared_error: 0.9746 - val_loss: 0.9452 - val_mean_squared_error: 0.9452\n",
      "Epoch 54/100\n",
      "396/396 [==============================] - 0s 51us/step - loss: 0.9743 - mean_squared_error: 0.9743 - val_loss: 0.9467 - val_mean_squared_error: 0.9467\n",
      "Epoch 55/100\n",
      "396/396 [==============================] - 0s 63us/step - loss: 0.9733 - mean_squared_error: 0.9733 - val_loss: 0.9445 - val_mean_squared_error: 0.9445\n",
      "Epoch 56/100\n",
      "396/396 [==============================] - 0s 68us/step - loss: 0.9730 - mean_squared_error: 0.9730 - val_loss: 0.9446 - val_mean_squared_error: 0.9446\n",
      "Epoch 57/100\n",
      "396/396 [==============================] - 0s 61us/step - loss: 0.9725 - mean_squared_error: 0.9725 - val_loss: 0.9467 - val_mean_squared_error: 0.9467\n",
      "Epoch 58/100\n",
      "396/396 [==============================] - 0s 61us/step - loss: 0.9724 - mean_squared_error: 0.9724 - val_loss: 0.9458 - val_mean_squared_error: 0.9458\n",
      "Epoch 59/100\n",
      "396/396 [==============================] - 0s 71us/step - loss: 0.9725 - mean_squared_error: 0.9725 - val_loss: 0.9462 - val_mean_squared_error: 0.9462\n",
      "Epoch 60/100\n",
      "396/396 [==============================] - 0s 73us/step - loss: 0.9711 - mean_squared_error: 0.9711 - val_loss: 0.9443 - val_mean_squared_error: 0.9443\n",
      "Epoch 61/100\n",
      "396/396 [==============================] - 0s 61us/step - loss: 0.9704 - mean_squared_error: 0.9704 - val_loss: 0.9428 - val_mean_squared_error: 0.9428\n",
      "Epoch 62/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: 0.9698 - mean_squared_error: 0.9698 - val_loss: 0.9417 - val_mean_squared_error: 0.9417\n",
      "Epoch 63/100\n",
      "396/396 [==============================] - 0s 86us/step - loss: 0.9696 - mean_squared_error: 0.9696 - val_loss: 0.9438 - val_mean_squared_error: 0.9438\n",
      "Epoch 64/100\n",
      "396/396 [==============================] - 0s 71us/step - loss: 0.9690 - mean_squared_error: 0.9690 - val_loss: 0.9438 - val_mean_squared_error: 0.9438\n",
      "Epoch 65/100\n",
      "396/396 [==============================] - 0s 63us/step - loss: 0.9687 - mean_squared_error: 0.9687 - val_loss: 0.9414 - val_mean_squared_error: 0.9414\n",
      "Epoch 66/100\n",
      "396/396 [==============================] - 0s 61us/step - loss: 0.9691 - mean_squared_error: 0.9691 - val_loss: 0.9413 - val_mean_squared_error: 0.9413\n",
      "Epoch 67/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: 0.9677 - mean_squared_error: 0.9677 - val_loss: 0.9410 - val_mean_squared_error: 0.9410\n",
      "Epoch 68/100\n",
      "396/396 [==============================] - 0s 83us/step - loss: 0.9674 - mean_squared_error: 0.9674 - val_loss: 0.9413 - val_mean_squared_error: 0.9413\n",
      "Epoch 69/100\n",
      "396/396 [==============================] - 0s 58us/step - loss: 0.9668 - mean_squared_error: 0.9668 - val_loss: 0.9406 - val_mean_squared_error: 0.9406\n",
      "Epoch 70/100\n",
      "396/396 [==============================] - 0s 68us/step - loss: 0.9662 - mean_squared_error: 0.9662 - val_loss: 0.9402 - val_mean_squared_error: 0.9402\n",
      "Epoch 71/100\n",
      "396/396 [==============================] - 0s 63us/step - loss: 0.9656 - mean_squared_error: 0.9656 - val_loss: 0.9401 - val_mean_squared_error: 0.9401\n",
      "Epoch 72/100\n",
      "396/396 [==============================] - 0s 78us/step - loss: 0.9647 - mean_squared_error: 0.9647 - val_loss: 0.9332 - val_mean_squared_error: 0.9332\n",
      "Epoch 73/100\n",
      "396/396 [==============================] - 0s 91us/step - loss: 0.9648 - mean_squared_error: 0.9648 - val_loss: 0.9367 - val_mean_squared_error: 0.9367\n",
      "Epoch 74/100\n",
      "396/396 [==============================] - 0s 83us/step - loss: 0.9645 - mean_squared_error: 0.9645 - val_loss: 0.9371 - val_mean_squared_error: 0.9371\n",
      "Epoch 75/100\n",
      "396/396 [==============================] - 0s 86us/step - loss: 0.9642 - mean_squared_error: 0.9642 - val_loss: 0.9379 - val_mean_squared_error: 0.9379\n",
      "Epoch 76/100\n",
      "396/396 [==============================] - 0s 71us/step - loss: 0.9630 - mean_squared_error: 0.9630 - val_loss: 0.9367 - val_mean_squared_error: 0.9367\n",
      "Epoch 77/100\n",
      "396/396 [==============================] - 0s 66us/step - loss: 0.9631 - mean_squared_error: 0.9631 - val_loss: 0.9376 - val_mean_squared_error: 0.9376\n",
      "Epoch 78/100\n",
      "396/396 [==============================] - 0s 63us/step - loss: 0.9621 - mean_squared_error: 0.9621 - val_loss: 0.9369 - val_mean_squared_error: 0.9369\n",
      "Epoch 79/100\n",
      "396/396 [==============================] - 0s 81us/step - loss: 0.9617 - mean_squared_error: 0.9617 - val_loss: 0.9371 - val_mean_squared_error: 0.9371\n",
      "Epoch 80/100\n",
      "396/396 [==============================] - 0s 78us/step - loss: 0.9615 - mean_squared_error: 0.9615 - val_loss: 0.9339 - val_mean_squared_error: 0.9339\n",
      "Epoch 81/100\n",
      "396/396 [==============================] - 0s 56us/step - loss: 0.9610 - mean_squared_error: 0.9610 - val_loss: 0.9358 - val_mean_squared_error: 0.9358\n",
      "Epoch 82/100\n",
      "396/396 [==============================] - 0s 58us/step - loss: 0.9607 - mean_squared_error: 0.9607 - val_loss: 0.9355 - val_mean_squared_error: 0.9355\n",
      "Epoch 83/100\n",
      "396/396 [==============================] - 0s 68us/step - loss: 0.9603 - mean_squared_error: 0.9603 - val_loss: 0.9322 - val_mean_squared_error: 0.9322\n",
      "Epoch 84/100\n",
      "396/396 [==============================] - 0s 61us/step - loss: 0.9594 - mean_squared_error: 0.9594 - val_loss: 0.9283 - val_mean_squared_error: 0.9283\n",
      "Epoch 85/100\n",
      "396/396 [==============================] - 0s 86us/step - loss: 0.9595 - mean_squared_error: 0.9595 - val_loss: 0.9303 - val_mean_squared_error: 0.9303\n",
      "Epoch 86/100\n",
      "396/396 [==============================] - 0s 51us/step - loss: 0.9584 - mean_squared_error: 0.9584 - val_loss: 0.9295 - val_mean_squared_error: 0.9295\n",
      "Epoch 87/100\n",
      "396/396 [==============================] - ETA: 0s - loss: 0.1742 - mean_squared_error: 0.17 - 0s 61us/step - loss: 0.9583 - mean_squared_error: 0.9583 - val_loss: 0.9290 - val_mean_squared_error: 0.9290\n",
      "Epoch 88/100\n",
      "396/396 [==============================] - 0s 71us/step - loss: 0.9581 - mean_squared_error: 0.9581 - val_loss: 0.9300 - val_mean_squared_error: 0.9300\n",
      "Epoch 89/100\n",
      "396/396 [==============================] - 0s 66us/step - loss: 0.9578 - mean_squared_error: 0.9578 - val_loss: 0.9321 - val_mean_squared_error: 0.9321\n",
      "Epoch 90/100\n",
      "396/396 [==============================] - 0s 61us/step - loss: 0.9564 - mean_squared_error: 0.9564 - val_loss: 0.9353 - val_mean_squared_error: 0.9353\n",
      "Epoch 91/100\n",
      "396/396 [==============================] - 0s 78us/step - loss: 0.9563 - mean_squared_error: 0.9563 - val_loss: 0.9376 - val_mean_squared_error: 0.9376\n",
      "Epoch 92/100\n",
      "396/396 [==============================] - 0s 68us/step - loss: 0.9554 - mean_squared_error: 0.9554 - val_loss: 0.9385 - val_mean_squared_error: 0.9385\n",
      "Epoch 93/100\n",
      "396/396 [==============================] - 0s 66us/step - loss: 0.9557 - mean_squared_error: 0.9557 - val_loss: 0.9377 - val_mean_squared_error: 0.9377\n",
      "Epoch 94/100\n",
      "396/396 [==============================] - 0s 63us/step - loss: 0.9547 - mean_squared_error: 0.9547 - val_loss: 0.9389 - val_mean_squared_error: 0.9389\n",
      "Epoch 95/100\n",
      "396/396 [==============================] - 0s 66us/step - loss: 0.9542 - mean_squared_error: 0.9542 - val_loss: 0.9364 - val_mean_squared_error: 0.9364\n",
      "Epoch 96/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: 0.9542 - mean_squared_error: 0.9542 - val_loss: 0.9343 - val_mean_squared_error: 0.9343\n",
      "Epoch 97/100\n",
      "396/396 [==============================] - 0s 71us/step - loss: 0.9537 - mean_squared_error: 0.9537 - val_loss: 0.9321 - val_mean_squared_error: 0.9321\n",
      "Epoch 98/100\n",
      "396/396 [==============================] - 0s 78us/step - loss: 0.9537 - mean_squared_error: 0.9537 - val_loss: 0.9339 - val_mean_squared_error: 0.9339\n",
      "Epoch 99/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "396/396 [==============================] - 0s 58us/step - loss: 0.9531 - mean_squared_error: 0.9531 - val_loss: 0.9355 - val_mean_squared_error: 0.9355\n",
      "Epoch 100/100\n",
      "396/396 [==============================] - 0s 76us/step - loss: 0.9529 - mean_squared_error: 0.9529 - val_loss: 0.9317 - val_mean_squared_error: 0.9317\n"
     ]
    }
   ],
   "source": [
    "#Your code here; rebuild a simple model using a relu layer followed by a linear layer. (See our code snippet above!)\n",
    "np.random.seed(123)\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(8, input_dim=12, activation='relu'))\n",
    "model.add(layers.Dense(1, activation = 'linear'))\n",
    "\n",
    "model.compile(optimizer= \"sgd\" ,loss='mse',metrics=['mse'])\n",
    "hist = model.fit(X_train, Y_train, batch_size=32, \n",
    "                 epochs=100, validation_data = (X_val, Y_val), verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's recheck our loss function. Not only should it be populated with numerical data as opposed to null values, but we also should expect to see the loss function decreasing with successive epochs, demonstrating optimization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.4367508822017245,\n",
       " 1.1737600450563912,\n",
       " 1.1074431744789837,\n",
       " 1.0811463829242822,\n",
       " 1.0674823071769994,\n",
       " 1.0551507299897647,\n",
       " 1.0471633648631549,\n",
       " 1.0402251433844518,\n",
       " 1.0351927098300722,\n",
       " 1.0305905367689903]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist.history['loss'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We have a converged model. With that, let's investigate how well the model performed with our good old friend, mean squarred error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE_train: 0.9510995526351265\n",
      "MSE_val: 0.931714990156479\n"
     ]
    }
   ],
   "source": [
    "pred_train = model.predict(X_train).reshape(-1)\n",
    "pred_val = model.predict(X_val).reshape(-1)  \n",
    "\n",
    "MSE_train = np.mean((pred_train-Y_train)**2)\n",
    "MSE_val = np.mean((pred_val-Y_val)**2)\n",
    "\n",
    "print(\"MSE_train:\", MSE_train)\n",
    "print(\"MSE_val:\", MSE_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    4.950000e+02\n",
       "mean    -1.525155e-17\n",
       "std      1.001012e+00\n",
       "min     -5.527742e-01\n",
       "25%     -3.768941e-01\n",
       "50%     -2.411269e-01\n",
       "75%      2.732173e-02\n",
       "max      1.540604e+01\n",
       "Name: like, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a36f438>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Y.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Weight Initializers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  He Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try and use a weight initializer. In the lecture, we've seen the He normalizer, which initializes the weight vector to have an average 0 and a variance of 2/n, with $n$ the number of features feeding into a layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(8, input_dim=12, kernel_initializer= \"he_normal\",\n",
    "                activation='relu'))\n",
    "model.add(layers.Dense(1, activation = 'linear'))\n",
    "\n",
    "model.compile(optimizer= \"sgd\" ,loss='mse',metrics=['mse'])\n",
    "hist = model.fit(X_train, Y_train, batch_size=32, \n",
    "                 epochs=100, validation_data = (X_val, Y_val),verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = model.predict(X_train).reshape(-1)\n",
    "pred_val = model.predict(X_val).reshape(-1)\n",
    "\n",
    "MSE_train = np.mean((pred_train-Y_train)**2)\n",
    "MSE_val = np.mean((pred_val-Y_val)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.924555923091105\n",
      "0.9991927982052867\n"
     ]
    }
   ],
   "source": [
    "print(MSE_train)\n",
    "print(MSE_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initializer does not really help us to decrease the MSE. We know that initializers can be particularly helpful in deeper networks, and our network isn't very deep. What if we use the `Lecun` initializer with a `tanh` activation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecun Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(8, input_dim=12, \n",
    "                kernel_initializer= \"lecun_normal\", activation='tanh'))\n",
    "model.add(layers.Dense(1, activation = 'linear'))\n",
    "\n",
    "model.compile(optimizer= \"sgd\" ,loss='mse',metrics=['mse'])\n",
    "hist = model.fit(X_train, Y_train, batch_size=32, \n",
    "                 epochs=100, validation_data = (X_val, Y_val), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = model.predict(X_train).reshape(-1)\n",
    "pred_val = model.predict(X_val).reshape(-1)\n",
    "\n",
    "MSE_train = np.mean((pred_train-Y_train)**2)\n",
    "MSE_val = np.mean((pred_val-Y_val)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9451289998621167\n",
      "0.9866754226484686\n"
     ]
    }
   ],
   "source": [
    "print(MSE_train)\n",
    "print(MSE_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not much of a difference, but a useful note to consider when tuning your network. Next, let's investigate the impace of various optimization algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(8, input_dim=12, activation='relu'))\n",
    "model.add(layers.Dense(1, activation = 'linear'))\n",
    "\n",
    "model.compile(optimizer= \"rmsprop\" ,loss='mse',metrics=['mse'])\n",
    "hist = model.fit(X_train, Y_train, batch_size=32, \n",
    "                 epochs=100, validation_data = (X_val, Y_val), verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = model.predict(X_train).reshape(-1)\n",
    "pred_val = model.predict(X_val).reshape(-1)\n",
    "\n",
    "MSE_train = np.mean((pred_train-Y_train)**2)\n",
    "MSE_val = np.mean((pred_val-Y_val)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9187002476363131\n",
      "0.9075237502261114\n"
     ]
    }
   ],
   "source": [
    "print(MSE_train)\n",
    "print(MSE_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(8, input_dim=12, activation='relu'))\n",
    "model.add(layers.Dense(1, activation = 'linear'))\n",
    "\n",
    "model.compile(optimizer= \"Adam\" ,loss='mse',metrics=['mse'])\n",
    "hist = model.fit(X_train, Y_train, batch_size=32, \n",
    "                 epochs=100, validation_data = (X_val, Y_val), verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = model.predict(X_train).reshape(-1)\n",
    "pred_val = model.predict(X_val).reshape(-1)\n",
    "\n",
    "MSE_train = np.mean((pred_train-Y_train)**2)\n",
    "MSE_val = np.mean((pred_val-Y_val)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9205551777047531\n",
      "0.9036722777237326\n"
     ]
    }
   ],
   "source": [
    "print(MSE_train)\n",
    "print(MSE_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate Decay with Momentum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "sgd = optimizers.SGD(lr=0.03, decay=0.0001, momentum=0.9)\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(8, input_dim=12, activation='relu'))\n",
    "model.add(layers.Dense(1, activation = 'linear'))\n",
    "\n",
    "model.compile(optimizer= sgd ,loss='mse',metrics=['mse'])\n",
    "hist = model.fit(X_train, Y_train, batch_size=32, \n",
    "                 epochs=100, validation_data = (X_val, Y_val), verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = model.predict(X_train).reshape(-1)\n",
    "pred_val = model.predict(X_val).reshape(-1)\n",
    "\n",
    "MSE_train = np.mean((pred_train-Y_train)**2)\n",
    "MSE_val = np.mean((pred_val-Y_val)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8451498606459134\n",
      "0.9522933827115981\n"
     ]
    }
   ],
   "source": [
    "print(MSE_train)\n",
    "print(MSE_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "* https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Consumer_complaints.ipynb  \n",
    "\n",
    "* https://catalog.data.gov/dataset/consumer-complaint-database  \n",
    "\n",
    "* https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/  \n",
    "\n",
    "* https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/  \n",
    "\n",
    "* https://machinelearningmastery.com/regression-tutorial-keras-deep-learning-library-python/  \n",
    "\n",
    "* https://stackoverflow.com/questions/37232782/nan-loss-when-training-regression-network  \n",
    "\n",
    "* https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary  \n",
    "\n",
    "In this lab, we began to practice some of the concepts regarding normalization and optimization for neural networks. In the final lab for this section, you'll independently practice these concepts on your own in order to tune a model to predict individuals payments to loans."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
